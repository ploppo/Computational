{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONDCiT4Kc51kTdaad8VwuG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/virgimarca/Computational/blob/main/Copia_di_DGM_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUVR8YNb7lzI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "NUM_PIXELS = 10\n",
        "\n",
        "# Numerically stable implementations\n",
        "def logsoftmax(x):\n",
        "    m = torch.max(x, -1, keepdim=True).values\n",
        "    return x - m - torch.log(torch.exp(x - m).sum(-1, keepdim=True))\n",
        "\n",
        "def logsumexp(x):\n",
        "    m = x.max(-1).values\n",
        "    return m + torch.log(torch.exp(x - m[...,None]).sum(-1))\n",
        "\n",
        "class ImageTransformer(nn.Module):\n",
        "    \"\"\"ImageTransformer with DMOL or categorical distribution.\"\"\"\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.hparams = hparams\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hparams) for _ in range(hparams.nlayers)])\n",
        "        self.input_dropout = nn.Dropout(p=hparams.dropout)\n",
        "        if self.hparams.distr == \"dmol\": # Discretized mixture of logistic, for ordinal valued inputs\n",
        "            assert self.hparams.channels == 3, \"Only supports 3 channels for DML\"\n",
        "            size = (1, self.hparams.channels)\n",
        "            self.embedding_conv = nn.Conv2d(1, self.hparams.hidden_size,\n",
        "                                            kernel_size=size, stride=size)\n",
        "            # 10 = 1 + 2c + c(c-1)/2; if only 1 channel, then 3 total\n",
        "            depth = self.hparams.num_mixtures * 10\n",
        "            self.output_dense = nn.Linear(self.hparams.hidden_size, depth, bias=False)\n",
        "        elif self.hparams.distr == \"cat\": # Categorical\n",
        "            self.embeds = nn.Embedding(NUM_PIXELS * self.hparams.channels, self.hparams.hidden_size)\n",
        "            self.output_dense = nn.Linear(self.hparams.hidden_size, NUM_PIXELS, bias=True)\n",
        "        else:\n",
        "            raise ValueError(\"Only dmol or categorical distributions\")\n",
        "\n",
        "    # TODO: can probably cache this computation. (Be wary of shapes for train vs. predict)\n",
        "    def add_timing_signal(self, X, min_timescale=1.0, max_timescale=1.0e4):\n",
        "        num_dims = len(X.shape) - 2 # 2 corresponds to batch and hidden_size dimensions\n",
        "        num_timescales = self.hparams.hidden_size // (num_dims * 2)\n",
        "        log_timescale_increment = np.log(max_timescale / min_timescale) / (num_timescales - 1)\n",
        "        inv_timescales = min_timescale * torch.exp((torch.arange(num_timescales).float() * -log_timescale_increment))\n",
        "        inv_timescales = inv_timescales.to(X.device)\n",
        "        total_signal = torch.zeros_like(X) # Only for debugging purposes\n",
        "        for dim in range(num_dims):\n",
        "            length = X.shape[dim + 1] # add 1 to exclude batch dim\n",
        "            position = torch.arange(length).float().to(X.device)\n",
        "            scaled_time = position.view(-1, 1) * inv_timescales.view(1, -1)\n",
        "            signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], 1)\n",
        "            prepad = dim * 2 * num_timescales\n",
        "            postpad = self.hparams.hidden_size - (dim + 1) * 2 * num_timescales\n",
        "            signal = F.pad(signal, (prepad, postpad))\n",
        "            for _ in range(1 + dim):\n",
        "                signal = signal.unsqueeze(0)\n",
        "            for _ in range(num_dims - 1 - dim):\n",
        "                signal = signal.unsqueeze(-2)\n",
        "            X += signal\n",
        "            total_signal += signal\n",
        "        return X\n",
        "\n",
        "    def shift_and_pad_(self, X):\n",
        "        # Shift inputs over by 1 and pad\n",
        "        shape = X.shape\n",
        "        X = X.view(shape[0], shape[1] * shape[2], shape[3])\n",
        "        X = X[:,:-1,:]\n",
        "        X = F.pad(X, (0, 0, 1, 0)) # Pad second to last dimension\n",
        "        X = X.view(shape)\n",
        "        return X\n",
        "\n",
        "    def forward(self, X, sampling=False):\n",
        "        # Reshape inputs\n",
        "        if sampling:\n",
        "            curr_infer_length = X.shape[1]\n",
        "            row_size = self.hparams.image_size * self.hparams.channels\n",
        "            nrows = curr_infer_length // row_size + 1\n",
        "            X = F.pad(X, (0, nrows * row_size - curr_infer_length))\n",
        "            X = X.view(X.shape[0], -1, row_size)\n",
        "        else:\n",
        "            X = X.permute([0, 2, 3, 1]).contiguous()\n",
        "            X = X.view(X.shape[0], X.shape[1], X.shape[2] * X.shape[3]) # Flatten channels into width\n",
        "\n",
        "        # Inputs -> embeddings\n",
        "        if self.hparams.distr == \"dmol\":\n",
        "            # Create a \"channel\" dimension for the 1x3 convolution\n",
        "            # (NOTE: can apply a 1x1 convolution and not reshape, this is for consistency)\n",
        "            X = X.unsqueeze(1)\n",
        "            X = F.relu(self.embedding_conv(X))\n",
        "            X = X.permute([0, 2, 3, 1]) # move channels to the end\n",
        "        elif self.hparams.distr == \"cat\":\n",
        "            # Convert to indexes, and use separate embeddings for different channels\n",
        "            X = (X * (NUM_PIXELS - 1)).long()\n",
        "            channel_addition = (torch.tensor([0, 1, 2]) * NUM_PIXELS).to(X.device).repeat(X.shape[2] // 3).view(1, 1, -1)\n",
        "            X += channel_addition\n",
        "            X = self.embeds(X) * (self.hparams.hidden_size ** 0.5)\n",
        "\n",
        "        X = self.shift_and_pad_(X)\n",
        "        X = self.add_timing_signal(X)\n",
        "        shape = X.shape\n",
        "        X = X.view(shape[0], -1, shape[3])\n",
        "\n",
        "        X = self.input_dropout(X)\n",
        "        for layer in self.layers:\n",
        "            X = layer(X)\n",
        "        X = self.layers[-1].preprocess_(X) # NOTE: this is identity (exists to replicate tensorflow code)\n",
        "        X = self.output_dense(X).view(shape[:3] + (-1,))\n",
        "\n",
        "        if not sampling and self.hparams.distr == \"cat\": # Unpack the channels\n",
        "            X = X.view(X.shape[0], X.shape[1], X.shape[2] // self.hparams.channels, self.hparams.channels, X.shape[3])\n",
        "            X = X.permute([0, 3, 1, 2, 4])\n",
        "\n",
        "        return X\n",
        "\n",
        "    def split_to_dml_params(self, preds, targets=None, sampling=False):\n",
        "        nm = self.hparams.num_mixtures\n",
        "        mix_logits, locs, log_scales, coeffs = torch.split(preds, [nm, nm * 3, nm * 3, nm * 3], dim=-1)\n",
        "        new_shape = preds.shape[:-1] + (3, nm)\n",
        "        locs = locs.view(new_shape)\n",
        "        coeffs = torch.tanh(coeffs.view(new_shape))\n",
        "        log_scales = torch.clamp(log_scales.view(new_shape), min=-7.)\n",
        "        if not sampling:\n",
        "            targets = targets.unsqueeze(-1)\n",
        "            locs1 = locs[...,1,:] + coeffs[...,0,:] * targets[:,0,...]\n",
        "            locs2 = locs[...,2,:] + coeffs[...,1,:] * targets[:,0,...] + coeffs[...,2,:] * targets[:,1,...]\n",
        "            locs = torch.stack([locs[...,0,:], locs1, locs2], dim=-2)\n",
        "            return mix_logits, locs, log_scales\n",
        "        else:\n",
        "            return mix_logits, locs, log_scales, coeffs\n",
        "\n",
        "    # Modified from official PixCNN++ code\n",
        "    def dml_logp(self, logits, means, log_scales, targets):\n",
        "        targets = targets.unsqueeze(-1)\n",
        "        centered_x = targets - means\n",
        "        inv_stdv = torch.exp(-log_scales)\n",
        "        plus_in = inv_stdv * (centered_x + 1. / 255.)\n",
        "        cdf_plus = torch.sigmoid(plus_in)\n",
        "        min_in = inv_stdv * (centered_x - 1. / 255.)\n",
        "        cdf_min = torch.sigmoid(min_in)\n",
        "        log_cdf_plus = plus_in - F.softplus(plus_in)  # log probability for edge case of 0 (before scaling)\n",
        "        log_one_minus_cdf_min = -F.softplus(min_in)  # log probability for edge case of 255 (before scaling)\n",
        "        cdf_delta = cdf_plus - cdf_min  # probability for all other cases\n",
        "        mid_in = inv_stdv * centered_x\n",
        "        log_pdf_mid = mid_in - log_scales - 2. * F.softplus(\n",
        "            mid_in)  # log probability in the center of the bin, to be used in extreme cases (not actually used in our code)\n",
        "\n",
        "        # now select the right output: left edge case, right edge case, normal case, extremely low prob case (doesn't actually happen for us)\n",
        "        log_probs = torch.where(targets < -0.999, log_cdf_plus, torch.where(targets > 0.999, log_one_minus_cdf_min,\n",
        "                                                                torch.where(cdf_delta > 1e-5,\n",
        "                                                                         torch.log(torch.clamp(cdf_delta, min=1e-12)),\n",
        "                                                                         log_pdf_mid - np.log(127.5))))\n",
        "        log_probs = log_probs.sum(3) + logsoftmax(logits)\n",
        "        log_probs = logsumexp(log_probs)\n",
        "        return log_probs\n",
        "\n",
        "    # Assumes targets have been rescaled to [-1., 1.]\n",
        "    def loss(self, preds, targets):\n",
        "        if self.hparams.distr == \"dmol\":\n",
        "            # Assumes 3 channels. Input: [batch_size, height, width, 10 * 10]\n",
        "            logits, locs, log_scales = self.split_to_dml_params(preds, targets)\n",
        "            targets = targets.permute([0, 2, 3, 1])\n",
        "            log_probs = self.dml_logp(logits, locs, log_scales, targets)\n",
        "            return -log_probs\n",
        "        elif self.hparams.distr == \"cat\":\n",
        "            targets = (targets * (NUM_PIXELS - 1)).long()\n",
        "            ce = F.cross_entropy(preds.permute(0, 4, 1, 2, 3), targets, reduction='none')\n",
        "            return ce\n",
        "\n",
        "    def accuracy(self, preds, targets):\n",
        "        if self.hparams.distr == \"cat\":\n",
        "            targets = (targets * (NUM_PIXELS - 1)).long()\n",
        "            argmax_preds = torch.argmax(preds, dim=-1)\n",
        "            acc = torch.eq(argmax_preds, targets).float().sum() / np.prod(argmax_preds.shape)\n",
        "            return acc\n",
        "        else:\n",
        "            # Computing accuracy for dmol is more computationally intensive, so we skip it\n",
        "            return torch.zeros((1,))\n",
        "\n",
        "    def sample_from_dmol(self, outputs):\n",
        "        logits, locs, log_scales, coeffs = self.split_to_dml_params(outputs, sampling=True)\n",
        "        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) * (1. - 2 * 1e-5) + 1e-5))\n",
        "        sel = torch.argmax(logits + gumbel_noise, -1, keepdim=True)\n",
        "        one_hot = torch.zeros_like(logits).scatter_(-1, sel, 1).unsqueeze(-2)\n",
        "        locs = (locs * one_hot).sum(-1)\n",
        "        log_scales = (log_scales * one_hot).sum(-1)\n",
        "        coeffs = (coeffs * one_hot).sum(-1)\n",
        "        unif = torch.rand_like(log_scales) * (1. - 2 * 1e-5) + 1e-5\n",
        "        logistic_noise = torch.log(unif) - torch.log1p(-unif)\n",
        "        x = locs + torch.exp(log_scales) * logistic_noise\n",
        "        # NOTE: sampling analogously to pixcnn++, which clamps first, unlike image transformer\n",
        "        x0 = torch.clamp(x[..., 0], -1., 1.)\n",
        "        x1 = torch.clamp(x[..., 1] + coeffs[..., 0] * x0, -1., 1.)\n",
        "        x2 = torch.clamp(x[..., 2] + coeffs[..., 1] * x0 + coeffs[..., 2] * x1, -1., 1.)\n",
        "        x = torch.stack([x0, x1, x2], -1)\n",
        "        return x\n",
        "\n",
        "    def sample_from_cat(self, logits, argmax=False):\n",
        "        if argmax:\n",
        "            sel = torch.argmax(logits, -1, keepdim=False).float() / 255.\n",
        "        else:\n",
        "            gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) * (1. - 2 * 1e-5) + 1e-5))\n",
        "            sel = torch.argmax(logits + gumbel_noise, -1, keepdim=False).float() / 255.\n",
        "        return sel\n",
        "\n",
        "    def sample(self, n, device, argmax=False):\n",
        "        total_len = (self.hparams.image_size ** 2)\n",
        "        if self.hparams.distr == \"cat\":\n",
        "            total_len *= self.hparams.channels\n",
        "        samples = torch.zeros((n, 3)).to(device)\n",
        "        for curr_infer_length in tqdm(range(total_len)):\n",
        "            outputs = self.forward(samples, sampling=True)\n",
        "            outputs = outputs.view(n, -1, outputs.shape[-1])[:,curr_infer_length:curr_infer_length+1,:]\n",
        "            if self.hparams.distr == \"dmol\":\n",
        "                x = self.sample_from_dmol(outputs).squeeze()\n",
        "            elif self.hparams.distr == \"cat\":\n",
        "                x = self.sample_from_cat(outputs, argmax=argmax)\n",
        "            if curr_infer_length == 0:\n",
        "                samples = x\n",
        "            else:\n",
        "                samples = torch.cat([samples, x], 1)\n",
        "        samples = samples.view(n, self.hparams.image_size, self.hparams.image_size, self.hparams.channels)\n",
        "        samples = samples.permute(0, 3, 1, 2)\n",
        "        return samples\n",
        "\n",
        "    def sample_from_preds(self, preds, argmax=False):\n",
        "        if self.hparams.distr == \"dmol\":\n",
        "            samples = self.sample_from_dmol(preds)\n",
        "            samples = samples.permute(0, 3, 1, 2)\n",
        "        elif self.hparams.distr == \"cat\":\n",
        "            samples = self.sample_from_cat(preds, argmax=argmax)\n",
        "        return samples\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"Implements a single layer of an unconditional ImageTransformer\"\"\"\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.attn = Attn(hparams)\n",
        "        self.hparams = hparams\n",
        "        self.dropout = nn.Dropout(p=hparams.dropout)\n",
        "        self.layernorm_attn = nn.LayerNorm([self.hparams.hidden_size], eps=1e-6, elementwise_affine=True)\n",
        "        self.layernorm_ffn = nn.LayerNorm([self.hparams.hidden_size], eps=1e-6, elementwise_affine=True)\n",
        "        self.ffn = nn.Sequential(nn.Linear(self.hparams.hidden_size, self.hparams.filter_size, bias=True),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Linear(self.hparams.filter_size, self.hparams.hidden_size, bias=True))\n",
        "\n",
        "    def preprocess_(self, X):\n",
        "        return X\n",
        "\n",
        "    # Takes care of the \"postprocessing\" from tensorflow code with the layernorm and dropout\n",
        "    def forward(self, X):\n",
        "        X = self.preprocess_(X)\n",
        "        y = self.attn(X)\n",
        "        X = self.layernorm_attn(self.dropout(y) + X)\n",
        "        y = self.ffn(self.preprocess_(X))\n",
        "        X = self.layernorm_ffn(self.dropout(y) + X)\n",
        "        return X\n",
        "\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.hparams = hparams\n",
        "        self.kd = self.hparams.total_key_depth or self.hparams.hidden_size\n",
        "        self.vd = self.hparams.total_value_depth or self.hparams.hidden_size\n",
        "        self.q_dense = nn.Linear(self.hparams.hidden_size, self.kd, bias=False)\n",
        "        self.k_dense = nn.Linear(self.hparams.hidden_size, self.kd, bias=False)\n",
        "        self.v_dense = nn.Linear(self.hparams.hidden_size, self.vd, bias=False)\n",
        "        self.output_dense = nn.Linear(self.vd, self.hparams.hidden_size, bias=False)\n",
        "        assert self.kd % self.hparams.num_heads == 0\n",
        "        assert self.vd % self.hparams.num_heads == 0\n",
        "\n",
        "    def dot_product_attention(self, q, k, v, bias=None):\n",
        "        logits = torch.einsum(\"...kd,...qd->...qk\", k, q)\n",
        "        if bias is not None:\n",
        "            logits += bias\n",
        "        weights = F.softmax(logits, dim=-1)\n",
        "        return weights @ v\n",
        "\n",
        "    def forward(self, X):\n",
        "        q = self.q_dense(X)\n",
        "        k = self.k_dense(X)\n",
        "        v = self.v_dense(X)\n",
        "        # Split to shape [batch_size, num_heads, len, depth / num_heads]\n",
        "        q = q.view(q.shape[:-1] + (self.hparams.num_heads, self.kd // self.hparams.num_heads)).permute([0, 2, 1, 3])\n",
        "        k = k.view(k.shape[:-1] + (self.hparams.num_heads, self.kd // self.hparams.num_heads)).permute([0, 2, 1, 3])\n",
        "        v = v.view(v.shape[:-1] + (self.hparams.num_heads, self.vd // self.hparams.num_heads)).permute([0, 2, 1, 3])\n",
        "        q *= (self.kd // self.hparams.num_heads) ** (-0.5)\n",
        "\n",
        "        if self.hparams.attn_type == \"global\":\n",
        "            bias = -1e9 * torch.triu(torch.ones(X.shape[1], X.shape[1]), 1).to(X.device)\n",
        "            result = self.dot_product_attention(q, k, v, bias=bias)\n",
        "        elif self.hparams.attn_type == \"local_1d\":\n",
        "            len = X.shape[1]\n",
        "            blen = self.hparams.block_length\n",
        "            pad = (0, 0, 0, (-len) % self.hparams.block_length) # Append to multiple of block length\n",
        "            q = F.pad(q, pad)\n",
        "            k = F.pad(k, pad)\n",
        "            v = F.pad(v, pad)\n",
        "\n",
        "            bias = -1e9 * torch.triu(torch.ones(blen, blen), 1).to(X.device)\n",
        "            first_output = self.dot_product_attention(\n",
        "                q[:,:,:blen,:], k[:,:,:blen,:], v[:,:,:blen,:], bias=bias)\n",
        "\n",
        "            if q.shape[2] > blen:\n",
        "                q = q.view(q.shape[0], q.shape[1], -1, blen, q.shape[3])\n",
        "                k = k.view(k.shape[0], k.shape[1], -1, blen, k.shape[3])\n",
        "                v = v.view(v.shape[0], v.shape[1], -1, blen, v.shape[3])\n",
        "                local_k = torch.cat([k[:,:,:-1], k[:,:,1:]], 3) # [batch, nheads, (nblocks - 1), blen * 2, depth]\n",
        "                local_v = torch.cat([v[:,:,:-1], v[:,:,1:]], 3)\n",
        "                tail_q = q[:,:,1:]\n",
        "                bias = -1e9 * torch.triu(torch.ones(blen, 2 * blen), blen + 1).to(X.device)\n",
        "                tail_output = self.dot_product_attention(tail_q, local_k, local_v, bias=bias)\n",
        "                tail_output = tail_output.view(tail_output.shape[0], tail_output.shape[1], -1, tail_output.shape[4])\n",
        "                result = torch.cat([first_output, tail_output], 2)\n",
        "                result = result[:,:,:X.shape[1],:]\n",
        "            else:\n",
        "                result = first_output[:,:,:X.shape[1],:]\n",
        "\n",
        "        result = result.permute([0, 2, 1, 3]).contiguous()\n",
        "        result = result.view(result.shape[0:2] + (-1,))\n",
        "        result = self.output_dense(result)\n",
        "        return result"
      ]
    }
  ]
}